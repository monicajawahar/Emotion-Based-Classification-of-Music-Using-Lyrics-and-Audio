{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lyrics Classification",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh_6DxElDPWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7dda24e-cd98-4fb4-f4a8-f2e6151fd799"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neovlZqv1VEp"
      },
      "source": [
        "!pip install xlutils\n",
        "import xlrd\n",
        "import xlwt\n",
        "from xlwt import Workbook\n",
        "from xlutils.copy import copy\n",
        "\n",
        "test = '/content/drive/My Drive/Train.xlsx' \n",
        "workbook = xlrd.open_workbook(test)\n",
        "sheet = workbook.sheet_by_index(0)\n",
        "wb = copy(workbook)\n",
        "s = wb.get_sheet(0)\n",
        "\n",
        "for rowNumber in range(1,10246):\n",
        "    v = sheet.cell_value(rowNumber,0)\n",
        "    a = sheet.cell_value(rowNumber,1) \n",
        "    va = (v,a)\n",
        "    s.write(rowNumber,0,str(va))\n",
        "    wb.save(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIxx03WxrFUN"
      },
      "source": [
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "#Import all the dependencies\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "REMOVE_STOPWORDS = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J87HE4ZrdEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "outputId": "123f2e96-a47f-4a2d-f76c-958e5a6b401c"
      },
      "source": [
        "import xlrd\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd\n",
        "from string import digits\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def stemSentence(filtered_sentence):\n",
        "    token_words=word_tokenize(filtered_sentence)\n",
        "    stem_sentence=[]\n",
        "    for word in token_words:\n",
        "        stem_sentence.append(porter.stem(word))\n",
        "        stem_sentence.append(\" \")\n",
        "    return \"\".join(stem_sentence)\n",
        "\n",
        "test = \"/content/drive/My Drive/Test.xlsx\"\n",
        "train = \"/content/drive/My Drive/Train.xlsx\"\n",
        "\n",
        "\n",
        "remove_digits = str.maketrans('', '', digits) \n",
        "symbols = \"!\\\"#$%&()*+-.,/:;<=>?@[\\]^_`{|}~\\n\"\n",
        "\n",
        "train1 = pd.read_excel(test)\n",
        "print(train1)\n",
        "\n",
        "train2 = pd.read_excel(train)\n",
        "#print(train2)\n",
        "\n",
        "df = train1.append(train2, ignore_index=True)\n",
        "#df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                      VA                                             Lyrics\n",
            "0       (-0.420759114214, 0.75464367831)  I let the beast in too soon\\nI don't know how ...\n",
            "1     (-0.465329712646, 0.0110058921692)  So be it, I'm your crowbar\\nIf that's what I a...\n",
            "2       (0.37332522108, -0.923150665621)  I can drink a whole Henessey fifth\\nSome call ...\n",
            "3      (0.894528341497, -0.390773614181)  I certainly haven't been shopping for any new ...\n",
            "4      (-1.63689972852, -0.459145270141)  Through the back window of our '59 wagon\\nI wa...\n",
            "...                                  ...                                                ...\n",
            "3139     (-1.70057201199, 1.84244440679)  What I really hate is music like yours\\nMakes ...\n",
            "3140      (1.17832480497, 1.18331091453)  As you brush your shoes, stand before the mirr...\n",
            "3141    (0.37332522108, -0.923150665621)  You can take all the tea in China\\nPut it in a...\n",
            "3142    (0.815392789182, 0.662457175897)  Once in a blue moon\\nSomething good comes alon...\n",
            "3143      (1.14285024704, 1.07115066993)  Your the one that I adore, I adore your the\\nY...\n",
            "\n",
            "[3144 rows x 2 columns]\n",
            "WARNING *** file size (14363879) not 512 + multiple of sector size (512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "CompDocError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCompDocError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-5e91bca69f85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m#print(train2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mformatting_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatting_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mon_demand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_demand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mragged_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mragged_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         )\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/book.py\u001b[0m in \u001b[0;36mopen_workbook_xls\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mformatting_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatting_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mon_demand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_demand\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mragged_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mragged_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             )\n\u001b[1;32m     89\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/book.py\u001b[0m in \u001b[0;36mbiff2_8_load\u001b[0;34m(self, filename, file_contents, logfile, verbosity, use_mmap, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilestr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m             \u001b[0mcd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mUSE_FANCY_CD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mqname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Workbook'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Book'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/compdoc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mem, logfile, DEBUG)\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mCompDocError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0msid\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCompDocError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSAT extension: invalid sector id: %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCompDocError\u001b[0m: MSAT extension: accessing sector 28056 but only 28054 in file"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iqKosLvWWX_"
      },
      "source": [
        "for rowNumber in range(1,13391):\n",
        "    F = sheet.cell_value(rowNumber,8)\n",
        "    wo_digits = F.translate(remove_digits)\n",
        "    F = word_tokenize(F)\n",
        "    F = np.char.lower(F)\n",
        "    for i in symbols:\n",
        "        F = np.char.replace(F, i, '')\n",
        "    F = np.char.replace(F, \"'\", \"\")\n",
        "    F = [i for i in F if i]\n",
        "    \n",
        "    word_tokens = F\n",
        "    stop_words = set(stopwords.words('english'))  \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
        "    \n",
        "    filtered_sentence = [] \n",
        "    \n",
        "    for w in F: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w) \n",
        "    #print(filtered_sentence) \n",
        "    \n",
        "    #print(nltk.pos_tag(filtered_sentence))\n",
        "    \n",
        "    lemmatizer = WordNetLemmatizer() \n",
        "    str1 = \" \"  \n",
        "    str1 = str1.join(filtered_sentence)\n",
        "    str1 = lemmatizer.lemmatize(str1)\n",
        "    #print(str1)\n",
        "    \n",
        "    x=stemSentence(str1)\n",
        "    #print(x)\n",
        "    \n",
        "    fdist = FreqDist(filtered_sentence)\n",
        "    #print(fdist)\n",
        "    #fdist.plot(30,cumulative=False)\n",
        "    cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1))\n",
        "    text_counts= cv.fit_transform(filtered_sentence)\n",
        "\n",
        "    tf=TfidfVectorizer()\n",
        "    text_tf= tf.fit_transform(filtered_sentence)\n",
        "\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf.fit(filtered_sentence)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVtcz79y5bwf"
      },
      "source": [
        "import sklearn\n",
        "# Import all of the scikit learn stuff\n",
        "from __future__ import print_function\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "import pandas as pd\n",
        "import warnings\n",
        "# Suppress warnings from pandas library\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning,\n",
        "module=\"pandas\", lineno=570)\n",
        "import numpy\n",
        "vectorizer = CountVectorizer(min_df = 1, stop_words = 'english')\n",
        "dtm = vectorizer.fit_transform(filtered_sentence)\n",
        "pd.DataFrame(dtm.toarray(),index=filtered_sentence,columns=vectorizer.get_feature_names\n",
        "()).head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZsnwfFU5eKJ"
      },
      "source": [
        "#Import Library\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Load Train and Test datasets\n",
        "#Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
        "train=pd.read_excel('/content/drive/My Drive/Colab Notebooks/train.xlsx')\n",
        "train_y=train['Lyrics']\n",
        "train_x=train.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "test=pd.read_excel('/content/drive/My Drive/Colab Notebooks/test.xlsx')\n",
        "test_y=test['Lyrics']\n",
        "test_x=test.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "# Create Linear SVM object\n",
        "support = svm.LinearSVC(random_state=20)\n",
        "\n",
        "# Train the model using the training sets and check score on test dataset\n",
        "support.fit(train_x, train_y)\n",
        "predicted= support.predict(test_x)\n",
        "score=accuracy_score(test_y,test_x)\n",
        "print(\"Accuracy Score is\", score )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLGU7OlC5zgR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb2aa8c-c3ca-4200-f63b-191a03520300"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')\n",
        "features = (np.random.randint(10, size=(100, 1)))\n",
        "print(features.shape)\n",
        "training_dataset_length = math.ceil(len(features) * .75)\n",
        "print(training_dataset_length)\n",
        "#Scale the all of the data to be values between 0 and 1 \n",
        "scaler = MinMaxScaler(feature_range=(0, 1)) \n",
        "scaled_data = scaler.fit_transform(features)\n",
        "train_data = scaled_data[0:training_dataset_length  , : ]\n",
        "\n",
        "#Splitting the data\n",
        "x_train=[]\n",
        "y_train = []\n",
        "#Load Train and Test datasets\n",
        "#Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
        "train=pd.read_excel('/content/drive/My Drive/Colab Notebooks/train.xlsx')\n",
        "train_y=train['Lyrics']\n",
        "train_x=train.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "test=pd.read_excel('/content/drive/My Drive/Colab Notebooks/test.xlsx')\n",
        "test_y=test['Lyrics']\n",
        "test_x=test.drop([\"Lyrics\"],axis=1)\n",
        "for i in range(10, len(train_data)):\n",
        "    x_train.append(train_data[i-10:i,0])\n",
        "    y_train.append(train_data[i,0])\n",
        "#Convert to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and and Dropout layer\n",
        "model.add(LSTM(units = 50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "# For Full connection layer we use dense\n",
        "# As the output is 1D so we use unit=1\n",
        "model.add(Dense(units = 1))\n",
        "#compile and fit the model on 30 epochs\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(x_train, y_train, epochs = 30, batch_size = 50)\n",
        "#Test data set\n",
        "test_data = scaled_data[training_dataset_length - 10: , : ]\n",
        "\n",
        "#splitting the x_test and y_test data sets\n",
        "x_test = []\n",
        "y_test =  features[training_dataset_length : , : ] \n",
        "\n",
        "for i in range(10,len(test_data)):\n",
        "    x_test.append(test_data[i-10:i,0])\n",
        "    \n",
        "#Convert x_test to a numpy array \n",
        "x_test = np.array(x_test)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
        "#check predicted values\n",
        "predictions = model.predict(x_test) \n",
        "#Undo scaling\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "#Calculate RMSE score\n",
        "#smaller the better \n",
        "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
        "rmse\n",
        "#Root mean square error (RMSE) is a method of measuring the difference between values predicted by a model and their actual values.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1)\n",
            "75\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3454\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2897\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2208\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1577\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1202\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1283\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1260\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1089\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1151\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1214\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1176\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1160\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1110\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1066\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1103\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1083\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1144\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1087\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1192\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1154\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1134\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1156\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1101\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1126\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1151\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1059\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1126\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1030\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1098\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1062\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4e70696d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2687180198906707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFJnnvrnqmIA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpmsSSLcqmsE",
        "outputId": "6cb2aa8c-c3ca-4200-f63b-191a03520300"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM\n",
        "import matplotlib.pyplot as plt\n",
        "#Splitting the data\n",
        "x_train=[]\n",
        "y_train = []\n",
        "#Load Train and Test datasets\n",
        "#Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
        "train=pd.read_excel('/content/drive/My Drive/Colab Notebooks/train.xlsx')\n",
        "train_y=train['Lyrics']\n",
        "train_x=train.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "test=pd.read_excel('/content/drive/My Drive/Colab Notebooks/test.xlsx')\n",
        "test_y=test['Lyrics']\n",
        "test_x=test.drop([\"Lyrics\"],axis=1)\n",
        "for i in range(10, len(train_data)):\n",
        "    x_train.append(train_data[i-10:i,0])\n",
        "    y_train.append(train_data[i,0])\n",
        "#Convert to numpy arrays\n",
        "x_train, y_train = np.array(x_train), np.array(y_train)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
        "from keras.layers import Dropout\n",
        "\n",
        "# Initialising the RNN\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a second LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a third LSTM layer and Dropout layer\n",
        "model.add(LSTM(units = 50, return_sequences = True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding a fourth LSTM layer and and Dropout layer\n",
        "model.add(LSTM(units = 50))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Adding the output layer\n",
        "# For Full connection layer we use dense\n",
        "# As the output is 1D so we use unit=1\n",
        "model.add(Dense(units = 1))\n",
        "#compile and fit the model on 30 epochs\n",
        "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "model.fit(x_train, y_train, epochs = 30, batch_size = 50)\n",
        "#Test data set\n",
        "test_data = scaled_data[training_dataset_length - 10: , : ]\n",
        "\n",
        "#splitting the x_test and y_test data sets\n",
        "x_test = []\n",
        "y_test =  features[training_dataset_length : , : ] \n",
        "\n",
        "for i in range(10,len(test_data)):\n",
        "    x_test.append(test_data[i-10:i,0])\n",
        "    \n",
        "#Convert x_test to a numpy array \n",
        "x_test = np.array(x_test)\n",
        "\n",
        "#Reshape the data into 3-D array\n",
        "x_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))\n",
        "#check predicted values\n",
        "predictions = model.predict(x_test) \n",
        "#Undo scaling\n",
        "predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "#Calculate RMSE score\n",
        "#smaller the better \n",
        "rmse=np.sqrt(np.mean(((predictions- y_test)**2)))\n",
        "rmse\n",
        "#Root mean square error (RMSE) is a method of measuring the difference between values predicted by a model and their actual values.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 1)\n",
            "75\n",
            "Epoch 1/30\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3454\n",
            "Epoch 2/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2897\n",
            "Epoch 3/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2208\n",
            "Epoch 4/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1577\n",
            "Epoch 5/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1202\n",
            "Epoch 6/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1283\n",
            "Epoch 7/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1260\n",
            "Epoch 8/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1089\n",
            "Epoch 9/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1151\n",
            "Epoch 10/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1214\n",
            "Epoch 11/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1176\n",
            "Epoch 12/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1160\n",
            "Epoch 13/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1110\n",
            "Epoch 14/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1066\n",
            "Epoch 15/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1103\n",
            "Epoch 16/30\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1083\n",
            "Epoch 17/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1144\n",
            "Epoch 18/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1087\n",
            "Epoch 19/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1192\n",
            "Epoch 20/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1154\n",
            "Epoch 21/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1134\n",
            "Epoch 22/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1156\n",
            "Epoch 23/30\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1101\n",
            "Epoch 24/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1126\n",
            "Epoch 25/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1151\n",
            "Epoch 26/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1059\n",
            "Epoch 27/30\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1126\n",
            "Epoch 28/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1030\n",
            "Epoch 29/30\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1098\n",
            "Epoch 30/30\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1062\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4e70696d90> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.2687180198906707"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUyQ2k-O1x__"
      },
      "source": [
        "#Import Library\n",
        "from sklearn import svm\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#Load Train and Test datasets\n",
        "#Identify feature and response variable(s) and values must be numeric and numpy arrays\n",
        "train=pd.read_excel('/content/drive/My Drive/Colab Notebooks/train.xlsx')\n",
        "train_y=train['Lyrics']\n",
        "train_x=train.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "test=pd.read_excel('/content/drive/My Drive/Colab Notebooks/test.xlsx')\n",
        "test_y=test['Lyrics']\n",
        "test_x=test.drop([\"Lyrics\"],axis=1)\n",
        "\n",
        "# Create Linear SVM object\n",
        "support = svm.LinearSVC(random_state=20)\n",
        "\n",
        "# Train the model using the training sets and check score on test dataset\n",
        "support.fit(train_x, train_y)\n",
        "predicted= support.predict(test_x)\n",
        "score=accuracy_score(test_y,test_x)\n",
        "print(\"Accuracy Score is\", score )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}