{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lyrics","provenance":[],"collapsed_sections":[],"mount_file_id":"1he6gjhHdipQaL2AzQy9x1Ys6JIj_X8sH","authorship_tag":"ABX9TyMLukj7HOMrZSHw/hxJHs40"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wh_6DxElDPWI","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1603784083304,"user_tz":-330,"elapsed":77846,"user":{"displayName":"Music FYP","photoUrl":"","userId":"12124137204479689389"}},"outputId":"406984ff-9f10-47ea-91f9-49073f5748e9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"l5uU_Hp2VZsn"},"source":["Train Dataset Web Scraping"]},{"cell_type":"code","metadata":{"id":"3wdGh-ZdrNCQ"},"source":["!pip install requests\n","!pip install xlutils\n","import requests\n","import xlrd\n","import xlwt\n","import requests\n","from xlwt import Workbook\n","from xlutils.copy import copy\n","\n","train = '/content/drive/My Drive/train.xlsx' \n","workbook = xlrd.open_workbook(train)\n","sheet = workbook.sheet_by_index(0)\n","wb = copy(workbook)\n","s = wb.get_sheet(0)\n","\n","for rowNumber in range(1,10246):\n","    artist = str(sheet.cell_value(rowNumber,5))\n","    title = str(sheet.cell_value(rowNumber,6))\n","    artist = artist.replace(\" \",\"+\")\n","    title = title.replace(\" \",\"+\")\n","    URL = \"https://www.google.com/search?q=\"+title+\"+\"+artist+\"+\"+\"lyrics\"\n","    r = requests.get(url = URL)\n","    te = r.text\n","    start=-1\n","    for i in range(0, 5): \n","        start = te.find(\"BNeawe tAd8D AP7Wnd\", start + 1)\n","    start = start + 21\n","    end = te.find(\"<\",start)\n","    lyrics = str(te[start:end])\n","    s.write(rowNumber,8,lyrics)\n","    wb.save(train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZp6r2PxHglH"},"source":["Test Dataset Web Scraping"]},{"cell_type":"code","metadata":{"id":"QJk0x5XJHMus"},"source":["!pip install requests\n","!pip install xlutils\n","import requests\n","import xlrd\n","import xlwt\n","import requests\n","from xlwt import Workbook\n","from xlutils.copy import copy\n","\n","test = '/content/drive/My Drive/test.xlsx' \n","workbook = xlrd.open_workbook(test)\n","sheet = workbook.sheet_by_index(0)\n","wb = copy(workbook)\n","s = wb.get_sheet(0)\n","\n","for rowNumber in range(1,3145):\n","    artist = str(sheet.cell_value(rowNumber,5))\n","    title = str(sheet.cell_value(rowNumber,6))\n","    artist = artist.replace(\" \",\"+\")\n","    title = title.replace(\" \",\"+\")\n","    URL = \"https://www.google.com/search?q=\"+title+\"+\"+artist+\"+\"+\"lyrics\"\n","    r = requests.get(url = URL)\n","    te = r.text\n","    start=-1\n","    for i in range(0, 5): \n","        start = te.find(\"BNeawe tAd8D AP7Wnd\", start + 1)\n","    start = start + 21\n","    end = te.find(\"<\",start)\n","    lyrics = str(te[start:end])\n","    s.write(rowNumber,8,lyrics)\n","    wb.save(test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ily5BRXeHkH9"},"source":["Validation Dataset Web Scraping"]},{"cell_type":"code","metadata":{"id":"lVC8w8fGHR4V"},"source":["!pip install requests\n","!pip install xlutils\n","import requests\n","import xlrd\n","import xlwt\n","import requests\n","from xlwt import Workbook\n","from xlutils.copy import copy\n","\n","validation = '/content/drive/My Drive/validation.xlsx' \n","workbook = xlrd.open_workbook(validation)\n","sheet = workbook.sheet_by_index(0)\n","wb = copy(workbook)\n","s = wb.get_sheet(0)\n","\n","for rowNumber in range(1,3554):\n","    artist = str(sheet.cell_value(rowNumber,5))\n","    title = str(sheet.cell_value(rowNumber,6))\n","    artist = artist.replace(\" \",\"+\")\n","    title = title.replace(\" \",\"+\")\n","    URL = \"https://www.google.com/search?q=\"+title+\"+\"+artist+\"+\"+\"lyrics\"\n","    r = requests.get(url = URL)\n","    te = r.text\n","    start=-1\n","    for i in range(0, 5): \n","        start = te.find(\"BNeawe tAd8D AP7Wnd\", start + 1)\n","    start = start + 21\n","    end = te.find(\"<\",start)\n","    lyrics = str(te[start:end])\n","    s.write(rowNumber,8,lyrics)\n","    wb.save(validation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"neovlZqv1VEp"},"source":["!pip install xlutils\n","import xlrd\n","import xlwt\n","from xlwt import Workbook\n","from xlutils.copy import copy\n","\n","test = '/content/drive/My Drive/Train.xlsx' \n","workbook = xlrd.open_workbook(test)\n","sheet = workbook.sheet_by_index(0)\n","wb = copy(workbook)\n","s = wb.get_sheet(0)\n","\n","for rowNumber in range(1,10246):\n","    v = sheet.cell_value(rowNumber,0)\n","    a = sheet.cell_value(rowNumber,1) \n","    va = (v,a)\n","    s.write(rowNumber,0,str(va))\n","    wb.save(test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIxx03WxrFUN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605467106072,"user_tz":-330,"elapsed":1809,"user":{"displayName":"Music FYP","photoUrl":"","userId":"12124137204479689389"}},"outputId":"16296a40-3c5c-4bf0-ba46-60f50b3353fa"},"source":["from collections import Counter\n","import pandas as pd\n","import numpy as np\n","import os\n","import re\n","\n","#Import all the dependencies\n","\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","REMOVE_STOPWORDS = True"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8J87HE4ZrdEL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605502545180,"user_tz":-330,"elapsed":1440,"user":{"displayName":"Music FYP","photoUrl":"","userId":"12124137204479689389"}},"outputId":"040b0fff-db9d-443b-cbfa-dfea834b0a1d"},"source":["import xlrd\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","import numpy as np\n","import re\n","import pandas as pd\n","from string import digits\n","\n","from nltk.stem import WordNetLemmatizer,PorterStemmer\n","\n","from nltk.probability import FreqDist\n","import matplotlib.pyplot as plt\n","\n","from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n","from nltk.tokenize import RegexpTokenizer\n","\n","def stemSentence(filtered_sentence):\n","    token_words=word_tokenize(filtered_sentence)\n","    stem_sentence=[]\n","    for word in token_words:\n","        stem_sentence.append(porter.stem(word))\n","        stem_sentence.append(\" \")\n","    return \"\".join(stem_sentence)\n","\n","test = \"/content/drive/My Drive/Test.xlsx\"\n","train = \"/content/drive/My Drive/Train.xlsx\"\n","\n","\n","remove_digits = str.maketrans('', '', digits) \n","symbols = \"!\\\"#$%&()*+-.,/:;<=>?@[\\]^_`{|}~\\n\"\n","\n","train1 = pd.read_excel(test)\n","print(train1)\n","\n","train2 = pd.read_excel(train)\n","#print(train2)\n","\n","df = train1.append(train2, ignore_index=True)\n","#df"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                 valence                                             Lyrics\n","0      (-0.682725080497, 0.316757791845)  Gently hold our heads\\nGently hold our heads o...\n","1      (-1.93524985679, -0.655809808621)  There ain't nothin' I can do, or nothin' I can...\n","2       (0.815392789182, 0.662457175897)  You're out of touch\\nI'm out of time\\nBut I'm ...\n","3           (1.25746035728, 1.086515087)  Finally close the door\\nYou'd left open wide\\n...\n","4       (0.37332522108, -0.923150665621)  Night and stars above that shine so bright\\nTh...\n","...                                  ...                                                ...\n","10240                           -1.93525  I woke up this morning in a strange place\\nI l...\n","10241                            1.17832  Here you come again\\nJust when I've begun to g...\n","10242                            -1.4004  on leaving school immersed in philanthropic no...\n","10243                          0.0349525  A desert road from Vegas to nowhere\\nSome plac...\n","10244                           -1.93525  You run around town like a fool and you think ...\n","\n","[10245 rows x 2 columns]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KqEjvc7yZYY8"},"source":["\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import word_tokenize\n","from string import punctuation\n","\n","def remove_stopwords(row):     #Remove Stopwords from text\n","    tokens = word_tokenize(row)\n","    filtered_word = [word for word in tokens if not word in stop_words]\n","    return filtered_word\n","\n","\n","def make_string(row):      #Convert list into string\n","    new = ''\n","    for word in row:\n","        new = new + word\n","        new = new + ' '\n","    return new\n","\n","df['statement_clean'] = ''\n","df['statement_clean'] = df['text'].apply(lambda x: x.lower())\n","stop_words = set(stopwords.words('english'))\n","df['statement_clean'] = df['statement_clean'].apply(remove_stopwords)\n","df['statement_clean'] = df['statement_clean'].apply(make_string)\n","df['statement_clean'].replace(to_replace = '[^\\w\\s]',value='',inplace = True,regex = True )  #Remove punctuation\n","df['statement_clean'].replace(to_replace = '[\\d]',value='',inplace = True,regex = True )     #Remove digits\n","df['statement_clean'] = df['statement_clean'].apply(lambda x:\" \".join(x.split()))            #Remove duplicate white spaces\n","df['statement_clean'] = df['statement_clean'].apply(lambda x:\" \".join(word for word in x.split(' ') if len(word) > 1))   #Remove single letter words\n","print(df['statement_clean'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6iqKosLvWWX_"},"source":["\n","for rowNumber in range(1,2):\n","    F = sheet.cell_value(rowNumber,8)\n","    wo_digits = F.translate(remove_digits)\n","    F = word_tokenize(F)\n","    F = np.char.lower(F)\n","    for i in symbols:\n","        F = np.char.replace(F, i, '')\n","    F = np.char.replace(F, \"'\", \"\")\n","    F = [i for i in F if i]\n","    \n","    word_tokens = F\n","    stop_words = set(stopwords.words('english'))  \n","    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n","    \n","    filtered_sentence = [] \n","    \n","    for w in F: \n","        if w not in stop_words: \n","            filtered_sentence.append(w) \n","    #print(filtered_sentence) \n","    \n","    #print(nltk.pos_tag(filtered_sentence))\n","    \n","    lemmatizer = WordNetLemmatizer() \n","    str1 = \" \"  \n","    str1 = str1.join(filtered_sentence)\n","    str1 = lemmatizer.lemmatize(str1)\n","    #print(str1)\n","    \n","    x=stemSentence(str1)\n","    #print(x)\n","    \n","    fdist = FreqDist(filtered_sentence)\n","    #print(fdist)\n","    #fdist.plot(30,cumulative=False)\n","    cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1))\n","    text_counts= cv.fit_transform(filtered_sentence)\n","\n","    tf=TfidfVectorizer()\n","    text_tf= tf.fit_transform(filtered_sentence)\n","\n","    tfidf = TfidfVectorizer()\n","    tfidf.fit(filtered_sentence)\n"],"execution_count":null,"outputs":[]}]}